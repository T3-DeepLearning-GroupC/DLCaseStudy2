{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\anaconda3\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision numpy matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Skipping corrupted image 1MOGQBWogR8oJr1WgERi9g.jpg: cannot identify image file './a2_photos\\\\1MOGQBWogR8oJr1WgERi9g.jpg'\n",
      "Skipping corrupted image 5q-sAvIPl0yNeuAbNBPM1g.jpg: cannot identify image file './a2_photos\\\\5q-sAvIPl0yNeuAbNBPM1g.jpg'\n",
      "Skipping corrupted image 74upe0h6XxwgzqpdnAh_7Q.jpg: cannot identify image file './a2_photos\\\\74upe0h6XxwgzqpdnAh_7Q.jpg'\n",
      "Skipping corrupted image B7xR9CuhRpP52PoehQHVow.jpg: cannot identify image file './a2_photos\\\\B7xR9CuhRpP52PoehQHVow.jpg'\n",
      "Skipping corrupted image C6n0nKVbgLbYmxSiQ_bFsg.jpg: cannot identify image file './a2_photos\\\\C6n0nKVbgLbYmxSiQ_bFsg.jpg'\n",
      "Skipping corrupted image CA9z96gGA4y9QOes2Y9eGw.jpg: cannot identify image file './a2_photos\\\\CA9z96gGA4y9QOes2Y9eGw.jpg'\n",
      "Skipping corrupted image CBxmBYD_5CXIL_F-2PDqmA.jpg: cannot identify image file './a2_photos\\\\CBxmBYD_5CXIL_F-2PDqmA.jpg'\n",
      "Skipping corrupted image GPMWGVjuCsa6fadnZsEplw.jpg: cannot identify image file './a2_photos\\\\GPMWGVjuCsa6fadnZsEplw.jpg'\n",
      "Skipping corrupted image JGpfPj8VEvnq1B-Xqr3w-A.jpg: cannot identify image file './a2_photos\\\\JGpfPj8VEvnq1B-Xqr3w-A.jpg'\n",
      "Skipping corrupted image jU-dKl2Ye4L_5x602yoctQ.jpg: cannot identify image file './a2_photos\\\\jU-dKl2Ye4L_5x602yoctQ.jpg'\n",
      "Skipping corrupted image K6pfRNwGodm1m1gFVQlj-Q.jpg: cannot identify image file './a2_photos\\\\K6pfRNwGodm1m1gFVQlj-Q.jpg'\n",
      "Skipping corrupted image ke4ohxa93GJz0KH9H2kwsQ.jpg: cannot identify image file './a2_photos\\\\ke4ohxa93GJz0KH9H2kwsQ.jpg'\n",
      "Skipping corrupted image l2vR3PyVMF3pgIERdDEuiQ.jpg: cannot identify image file './a2_photos\\\\l2vR3PyVMF3pgIERdDEuiQ.jpg'\n",
      "Skipping corrupted image MduVueqYTBlEkX-axrh1ug.jpg: cannot identify image file './a2_photos\\\\MduVueqYTBlEkX-axrh1ug.jpg'\n",
      "Skipping corrupted image O0bVFyP58TOEix6IjERXQA.jpg: cannot identify image file './a2_photos\\\\O0bVFyP58TOEix6IjERXQA.jpg'\n",
      "Skipping corrupted image QRUo4vqUu3X9V4eIqBpY8A.jpg: cannot identify image file './a2_photos\\\\QRUo4vqUu3X9V4eIqBpY8A.jpg'\n",
      "Skipping corrupted image qxSXsYMA3aWuAfigeqeOOQ.jpg: cannot identify image file './a2_photos\\\\qxSXsYMA3aWuAfigeqeOOQ.jpg'\n",
      "Skipping corrupted image RLtBKD2rlfTaELWejmLBCA.jpg: cannot identify image file './a2_photos\\\\RLtBKD2rlfTaELWejmLBCA.jpg'\n",
      "Skipping corrupted image tlp6LCLDsvL1GjO_kW_plQ.jpg: cannot identify image file './a2_photos\\\\tlp6LCLDsvL1GjO_kW_plQ.jpg'\n",
      "Skipping corrupted image t_sV6mI4oNvbvohhZAyeuA.jpg: cannot identify image file './a2_photos\\\\t_sV6mI4oNvbvohhZAyeuA.jpg'\n",
      "Skipping corrupted image W94rrCn0O5K1lkfD26m4tw.jpg: cannot identify image file './a2_photos\\\\W94rrCn0O5K1lkfD26m4tw.jpg'\n",
      "Skipping corrupted image yFjqHyOaNFwzIWTV8EE9hg.jpg: cannot identify image file './a2_photos\\\\yFjqHyOaNFwzIWTV8EE9hg.jpg'\n",
      "Skipping corrupted image zTzdu2QqLozHpW_qYWF84w.jpg: cannot identify image file './a2_photos\\\\zTzdu2QqLozHpW_qYWF84w.jpg'\n",
      "Found 52760 valid images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "# Set device and create output directory\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directory for generated images\n",
    "os.makedirs(\"generated_images\", exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.0002\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "channels_img = 3  # RGB images\n",
    "z_dim = 100\n",
    "num_epochs = 10\n",
    "features_gen = 64\n",
    "features_disc = 64\n",
    "\n",
    "# Dataset class for loading Yelp images\n",
    "class YelpDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        \n",
    "        # Filter out invalid images during initialization\n",
    "        for f in os.listdir(image_folder):\n",
    "            if f.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                try:\n",
    "                    img_path = os.path.join(image_folder, f)\n",
    "                    with Image.open(img_path) as img:\n",
    "                        img.verify()  # Verify image integrity\n",
    "                    self.image_files.append(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping corrupted image {f}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} valid images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Keep trying until a valid image is loaded\n",
    "        while True:\n",
    "            try:\n",
    "                img_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {self.image_files[idx]}: {str(e)}\")\n",
    "                idx = (idx + 1) % len(self.image_files)  # Move to the next image\n",
    "\n",
    "# Transformations for preprocessing the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)]),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = YelpDataset(image_folder='./a2_photos', transform=transform)\n",
    "\n",
    "# Create data loader\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52760"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Generator class definition\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Define the layers of the generator using a sequential block\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(z_dim, features_g * 16, 4, 1, 0),  # 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # 32x32\n",
    "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=4, stride=2, padding=1),  # 64x64\n",
    "            nn.Tanh(),  # Output layer activation (scaled to [-1, 1])\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        # Helper function to create the block\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True),  # Use ReLU for activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Discriminator class definition\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Define the layers of the discriminator using a sequential block\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),  # 32x32\n",
    "            nn.LeakyReLU(0.2),  # Use LeakyReLU activation for better gradient flow\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),  # 16x16\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),  # 8x8\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),  # 4x4\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),  # 1x1\n",
    "            nn.Sigmoid(),  # Output layer with Sigmoid activation to output probabilities\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        # Helper function to create a block of layers for the discriminator\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),  # LeakyReLU with negative slope of 0.2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the discriminator\n",
    "        return self.disc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(z_dim, channels_img, features_gen).to(device)\n",
    "discriminator = Discriminator(channels_img, features_disc).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     fake_images \u001b[38;5;241m=\u001b[39m generator(noise)\n\u001b[0;32m     49\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(discriminator(fake_images))\n\u001b[1;32m---> 51\u001b[0m     g_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     52\u001b[0m     optimizer_g\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Save generated images\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Optimizers for the generator and discriminator\n",
    "optimizer_g = optim.RMSprop(generator.parameters(), lr=lr)\n",
    "optimizer_d = optim.RMSprop(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "n_critic = 5  # Number of training steps for the discriminator\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, real_images in enumerate(loader):\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        # Train the discriminator\n",
    "        for _ in range(n_critic):\n",
    "            optimizer_d.zero_grad()\n",
    "\n",
    "            # Real images\n",
    "            real_labels = torch.ones(real_images.size(0), 1).to(device)\n",
    "            real_output = discriminator(real_images)\n",
    "            d_loss_real = torch.mean(real_output)\n",
    "\n",
    "            # Fake images\n",
    "            noise = torch.randn(real_images.size(0), z_dim, 1, 1).to(device)\n",
    "            fake_images = generator(noise)\n",
    "            fake_labels = torch.zeros(real_images.size(0), 1).to(device)\n",
    "            fake_output = discriminator(fake_images.detach())\n",
    "            d_loss_fake = torch.mean(fake_output)\n",
    "\n",
    "            # Total loss\n",
    "            d_loss = d_loss_fake - d_loss_real\n",
    "            d_loss.backward()\n",
    "\n",
    "            # Weight clipping\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "            optimizer_d.step()\n",
    "\n",
    "        # Train the generator\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        noise = torch.randn(real_images.size(0), z_dim, 1, 1).to(device)\n",
    "        fake_images = generator(noise)\n",
    "        g_loss = -torch.mean(discriminator(fake_images))\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "    # Save generated images\n",
    "    if epoch % 1 == 0:\n",
    "        generated_images = generator(noise).detach().cpu()\n",
    "        save_image(generated_images, f\"generated_images/generated_epoch_{epoch}.png\", nrow=16, normalize=True)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the DataLoader is set up with num_workers\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "n_critic = 5  # Number of training steps for the discriminator\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, real_images in enumerate(loader):\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        # Train the discriminator\n",
    "        for _ in range(n_critic):\n",
    "            optimizer_d.zero_grad()\n",
    "\n",
    "            # Real images\n",
    "            real_output = discriminator(real_images)\n",
    "            d_loss_real = torch.mean(real_output)\n",
    "\n",
    "            # Fake images\n",
    "            noise = torch.randn(real_images.size(0), z_dim, 1, 1).to(device)\n",
    "            fake_images = generator(noise)\n",
    "            fake_output = discriminator(fake_images.detach())\n",
    "            d_loss_fake = torch.mean(fake_output)\n",
    "\n",
    "            # Total loss\n",
    "            d_loss = d_loss_fake - d_loss_real\n",
    "            d_loss.backward()\n",
    "\n",
    "            # Weight clipping\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "            optimizer_d.step()\n",
    "\n",
    "        # Train the generator\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        noise = torch.randn(real_images.size(0), z_dim, 1, 1).to(device)\n",
    "        fake_images = generator(noise)\n",
    "        g_loss = -torch.mean(discriminator(fake_images))\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "    # Save generated images periodically\n",
    "    if epoch % 1 == 0:\n",
    "        generated_images = generator(noise).detach().cpu()\n",
    "        save_image(generated_images, f\"generated_images/generated_epoch_{epoch}.png\", nrow=16, normalize=True)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
